{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import operator\n",
    "import json\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, scale, PolynomialFeatures\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, r2_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from pandas.plotting import scatter_matrix\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"Dataset/train.csv\", keep_default_na=False)\n",
    "test_csv = pd.read_csv(\"Dataset/test.csv\", keep_default_na=False)\n",
    "\n",
    "data = {}\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    dataset['Review Text'] = dataset[\"Review Title\"].map(str) + \" \" + dataset['Review Text']    \n",
    "    dataset = dataset.drop(['Review Title'],axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 5959, test = 2553\n"
     ]
    }
   ],
   "source": [
    "train = preprocess_data(train_csv)\n",
    "test = preprocess_data(test_csv)\n",
    "\n",
    "train_X = train[\"Review Text\"]\n",
    "train_y = train[\"topic\"]\n",
    "test_X = test[\"Review Text\"]\n",
    "\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train[\"Review Text\"], train[\"topic\"], random_state=33)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))\n",
    "#s = \" \".join(train[\"Review Text\"][0:10])\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def build_dict(data, vocab_size = 6300):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    word_counts = Counter(np.concatenate( data, axis=0 ))\n",
    "    # word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    print(len(sorted_words))\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_dict = build_dict(train_X)\n",
    "# print(word_dict)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    #words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless Did nothing for me, didn't help lost even with working out and eating healthy. Didn't curb appetite or anything.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['useless',\n",
       " 'nothing',\n",
       " 'help',\n",
       " 'lost',\n",
       " 'even',\n",
       " 'working',\n",
       " 'eating',\n",
       " 'healthy',\n",
       " 'curb',\n",
       " 'appetite',\n",
       " 'anything']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_X[0])\n",
    "review_to_words(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"key_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    print(cache_file)\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            print(\"File not found\")\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        print('Preprocess training and test data to obtain words for each review')\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        print('Preprocess training and test data to obtain words for each review')\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        print('Unpack data loaded from cache file')\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'])\n",
    "    \n",
    "    return words_train, words_test, labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_data.pkl\n",
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "Unpack data loaded from cache file\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y = preprocess_data(train_X, test_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8757\n"
     ]
    }
   ],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['taste', 'product', 'like', 'bad', 'brand']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# determine the five most frequently appearing words in the training set.\n",
    "word_counts = Counter(np.concatenate( train_X, axis=0 ))\n",
    "sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(sorted_words[0:5])\n",
    "print(word_counts['tast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_dict.json', 'w') as f:\n",
    "    json.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cache_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X[0]))\n",
    "print(train_X_len[0:1]) # 500 - 322 (non zeros) = 178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1).to_csv('sample_train.csv', header=False, index=False)\n",
    "pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1).to_csv('sample_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = 1\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=21)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.word_dict = None\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.08\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "        \n",
    "        # self.fc.bias.data.zero_()\n",
    "        self.dense.bias.data.fill_(0)\n",
    "        # self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.weight.data.normal_(0.0, (1.0 / np.sqrt(self.dense.in_features)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        \n",
    "        #print(reviews)\n",
    "        #print(batch_size)\n",
    "        \n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dense(lstm_out)\n",
    "        #print(out)\n",
    "        #sig_out = out.view(batch_size, -1, self.output_size)\n",
    "        #sig_out = sig_out[:, -1]\n",
    "        #print(sig_out)\n",
    "        \n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple DNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, input_size, output_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(DNNClassifier, self).__init__()\n",
    "\n",
    "        self.sig = nn.Sigmoid()        \n",
    "        # self.word_dict = None\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.init_weights()\n",
    "        self.soft = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def init_weights(m):\n",
    "        initrange = 0.08\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            # get the number of the inputs\n",
    "            n = m.in_features\n",
    "            y = 1.0/np.sqrt(n)\n",
    "            m.weight.data.normal_(0.0, y)\n",
    "            m.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        #input_x = input_x.t()\n",
    "        #lengths = input_x[0,:]        \n",
    "        x = input_x[0:,1:]\n",
    "        #print(x)\n",
    "        #print(input_size)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.out(x)\n",
    "        return self.sig(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 11  4 ...  1  1  1]\n",
      "[[505  92  88 ...   0   0   0]\n",
      " [505  92  88 ...   0   0   0]\n",
      " [378  13 456 ...   0   0   0]\n",
      " ...\n",
      " [ 24  68  34 ...   0   0   0]\n",
      " [315 128   2 ...   0   0   0]\n",
      " [  2   2  48 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv('sample_train.csv', header=None, names=None)\n",
    "\n",
    "#binary_label = pd.get_dummies(train_sample[[0]].astype('category'), prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "#binary_label_columns = binary_label.dot(binary_label.columns)\n",
    "#binary_label_columns = binary_label.idxmax(axis=1)\n",
    "\n",
    "#binary_label_stack = binary_label.stack()\n",
    "#print(binary_label.columns[0:20])\n",
    "#print(binary_label_columns)\n",
    "#print(binary_label_columns[5].replace(\"0_\", \"\"))\n",
    "\n",
    "#labels = np.asarray(binary_label)\n",
    "\n",
    "lbl = LabelEncoder() \n",
    "lbl.fit(list(train_sample[[0]].values)) \n",
    "# Turn the input pandas dataframe into tensors\n",
    "\n",
    "train_sample_y = lbl.transform(list(train_sample[[0]].values))\n",
    "#train_sample_y = labels\n",
    "train_sample_X = np.matrix(train_sample.drop([0,1], axis=1))\n",
    "#train_sample_X = np.ndarray(train_sample.drop([0], axis=1).values)\n",
    "\n",
    "#train_sample_y = torch.from_numpy(labels).float().squeeze()\n",
    "#train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "print(train_sample_y)\n",
    "print(train_sample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20335570469798658\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_sample_X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaler.transform(train_sample_X), train_sample_y, random_state=0)\n",
    "\n",
    "#modelsvm = DecisionTreeClassifier(max_depth = 2)\n",
    "modelsvm = svm.SVC(kernel = 'sigmoid', C = 1)\n",
    "modelsvm = modelsvm.fit(X_train, y_train)\n",
    "\n",
    "predicted_labels = modelsvm.predict(X_val)\n",
    "print(accuracy_score(y_val, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4808394630659807"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_val, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "# loading the iris dataset \n",
    "iris = datasets.load_iris() \n",
    "  \n",
    "# X -> features, y -> label \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "  \n",
    "# dividing X, y into train and test data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) \n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "# training a linear SVM classifier \n",
    "from sklearn.svm import SVC \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train) \n",
    "svm_predictions = svm_model_linear.predict(X_test) \n",
    "  \n",
    "# model accuracy for X_test   \n",
    "accuracy = svm_model_linear.score(X_test, y_test) \n",
    "  \n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(y_test, svm_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
