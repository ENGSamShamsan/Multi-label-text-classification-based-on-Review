{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/koushik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchtext import data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import operator\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "import re, string\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, scale, PolynomialFeatures\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, r2_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from torch.autograd import Variable\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "NLP = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"Dataset/train.csv\", keep_default_na=False)\n",
    "test_csv = pd.read_csv(\"Dataset/test.csv\", keep_default_na=False)\n",
    "\n",
    "#data = {}\n",
    "#labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def beautifyAllText(x):\n",
    "    if x != \"\":\n",
    "        text = BeautifulSoup(x, \"html.parser\").get_text() # Remove HTML tags\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "        words = text.split() # Split string into words\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "        words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "        return \" \".join(words)\n",
    "    return x\n",
    "\n",
    "def tokenizer(comment):\n",
    "    comment = BeautifulSoup(comment, \"html.parser\").get_text()\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "        str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    return [x.text for x in NLP.tokenizer(comment) if x.text != \" \"]\n",
    "\n",
    "def get_iterator(dataset, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    dataset_iter = data.Iterator(\n",
    "        dataset, batch_size=batch_size, device=0,\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    return dataset_iter\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    #dataset.sort_values(\"Review Text\", inplace = True) \n",
    "    #dataset.drop_duplicates(subset =\"Review Text\", keep=False,inplace=False)\n",
    "    #dataset = dataset.reset_index(drop=True)\n",
    "    dataset['Review Text'] = dataset[\"Review Title\"].map(str) + \" \" + dataset['Review Text']\n",
    "    dataset['Review Text'] = dataset[\"Review Text\"].apply(beautifyAllText)\n",
    "    #dataset = dataset.drop(['Review Title'],axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec:  62%|██████▏   | 4.07G/6.60G [1:07:19<4:18:30, 163kB/s]  "
     ]
    }
   ],
   "source": [
    "comment = data.Field(\n",
    "    sequential=True,\n",
    "    fix_length=100,\n",
    "    tokenize=tokenizer,\n",
    "    pad_first=True,\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "title = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "labelfield = data.Field(sequential=False, use_vocab=True)\n",
    "\n",
    "print(\"Reading train csv file...\")\n",
    "train = data.TabularDataset(\n",
    "        path='Dataset/train.csv', format='csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('Review Text', comment),\n",
    "            ('Review Title', title),\n",
    "            ('topic', labelfield)\n",
    "        ])\n",
    "\n",
    "test = data.TabularDataset(\n",
    "        path='Dataset/test.csv', format='csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('Review Text', comment),\n",
    "            ('Review Title', title)\n",
    "        ])\n",
    "\n",
    "comment.build_vocab(\n",
    "    train, test,\n",
    "    max_size=20000,\n",
    "    min_freq=50,\n",
    "    vectors=\"fasttext.en.300d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for examples in get_iterator(train, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    x = examples.comment_text # (fix_length, batch_size) Tensor\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 5959, test = 2553\n",
      "                                         Review Text  \\\n",
      "0  bad tast use chia seed protein shake tast like...   \n",
      "1  bad tast use chia seed protein shake tast like...   \n",
      "2                            chang result wast money   \n",
      "3  good vegan choic poor non vegan choic use book...   \n",
      "4  good vegan choic poor non vegan choic use book...   \n",
      "\n",
      "                               Review Title  \n",
      "0                                  Bad tast  \n",
      "1                                  Bad tast  \n",
      "2                    No change. No results.  \n",
      "3  Good Vegan Choice, Poor Non Vegan Choice  \n",
      "4  Good Vegan Choice, Poor Non Vegan Choice  \n"
     ]
    }
   ],
   "source": [
    "test_csv.drop_duplicates(subset =\"Review Text\", keep=False,inplace=False)\n",
    "test_csv = test_csv.reset_index(drop=True)\n",
    "\n",
    "train = preprocess_data(train_csv.copy())\n",
    "test = preprocess_data(test_csv.copy())\n",
    "\n",
    "train_val = train[0:1000]\n",
    "train_val.drop_duplicates(subset =\"Review Text\", keep=False,inplace=False)\n",
    "\n",
    "train_X = train[\"Review Text\"]\n",
    "train_y = train[\"topic\"]\n",
    "test_X = test[\"Review Text\"]\n",
    "\n",
    "lbl = LabelEncoder() \n",
    "lbl.fit(list(train_y.values))\n",
    "\n",
    "train_y_label = lbl.transform(train_y.values)\n",
    "\n",
    "train_val[\"topic\"] = lbl.transform(train_val[\"topic\"].values)\n",
    "\n",
    "binary_label = pd.get_dummies(train_y)\n",
    "\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train[\"Review Text\"], train[\"topic\"], random_state=33)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))\n",
    "#s = \" \".join(train[\"Review Text\"][0:10])\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'useless noth help lost even work eat healthi curb appetit anyth'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168.12384628293339, 161.0621470324028, 1665)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = train_X.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X.fillna(\"unknown\", inplace=True)\n",
    "#test_X.fillna(\"unknown\", inplace=True)\n",
    "#train_val[\"Review Text\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300\n",
      "6300\n"
     ]
    }
   ],
   "source": [
    "n = train_X.shape[0]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tweet_tokenizer.tokenize)\n",
    "\n",
    "full_text = list(train_X.values) + list(test_X.values)\n",
    "\n",
    "vectorizer.fit(full_text)\n",
    "\n",
    "train_vectorized = vectorizer.transform(train_X)\n",
    "test_vectorized = vectorizer.transform(test_X)\n",
    "\n",
    "\"\"\"\n",
    "vec = TfidfVectorizer(ngram_range=(1,5), smooth_idf=True, use_idf=True, sublinear_tf=1, max_features=6300, \n",
    "                      strip_accents='unicode' )    \n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "\n",
    "vec = vec.fit(pd.concat([train_X, test_X], axis=0))\n",
    "\n",
    "trn_term_doc = vec.transform(train_X)\n",
    "val_term_doc = vec.transform(train_val[\"Review Text\"])\n",
    "test_term_doc = vec.transform(test_X)\n",
    "\"\"\"\n",
    "\n",
    "# print(len(vectorizer.get_feature_names()))\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 6300)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trn_term_doc.shape)\n",
    "print(type(trn_term_doc))\n",
    "print(trn_term_doc.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc.toarray()\n",
    "test_term = test_term_doc.toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(np.array(x), np.array(binary_label), random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = 1\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim * 2, out_features=21)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.LogSoftmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.word_dict = None\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.08\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "        \n",
    "        # self.fc.bias.data.zero_()\n",
    "        self.dense.bias.data.fill_(0)\n",
    "        # self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.weight.data.normal_(0.0, (1.0 / np.sqrt(self.dense.in_features)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        \n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        \n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.dense(conc))\n",
    "        out = self.dropout(conc)\n",
    "        #out = self.out(out)\n",
    "        \"\"\"\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        print(out)\n",
    "        #sig_out = out.view(batch_size, -1, self.output_size)\n",
    "        #sig_out = sig_out[:, -1]\n",
    "        #print(sig_out)\n",
    "        \n",
    "        #out = out[lengths - 1, range(len(lengths))]\n",
    "        \"\"\"\n",
    "        return self.sig(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple DNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, input_size, output_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(DNNClassifier, self).__init__()\n",
    "\n",
    "        self.sig = nn.Sigmoid()        \n",
    "        # self.word_dict = None\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.init_weights()\n",
    "        self.soft = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def init_weights(m):\n",
    "        initrange = 0.08\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            # get the number of the inputs\n",
    "            n = m.in_features\n",
    "            y = 1.0/np.sqrt(n)\n",
    "            m.weight.data.normal_(0.0, y)\n",
    "            m.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        #input_x = input_x.t()\n",
    "        #lengths = input_x[0,:]        \n",
    "        x = input_x[0:,1:]\n",
    "        #print(x)\n",
    "        #print(input_size)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.out(x)\n",
    "        return self.sig(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 50\n",
    "num_epochs = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Model parameters\n",
    "# Input size\n",
    "input_size = X_train.shape[1] - 1\n",
    "# Output size\n",
    "output_size = 21\n",
    "# Embedding Dimension\n",
    "embedding_dim = 32\n",
    "# Hidden Dimension\n",
    "hidden_dim = 128\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "#vocabulary size\n",
    "vocab_size = 6300\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(y_train).float().squeeze())\n",
    "val_sample_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_val).long(), torch.from_numpy(y_val).float().squeeze())\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=batch_size)\n",
    "var_sample_dl = torch.utils.data.DataLoader(val_sample_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, optimizer, loss_fn, device):\n",
    "    train_loss = 0\n",
    "    val_loss = 0Da\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #batch_len = batch_X.t()[0,:]\n",
    "            #print(batch_len)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            output = model.forward(batch_X)\n",
    "            #print(output)\n",
    "            \n",
    "            #_, batch_y = batch_y.max(dim=1)            \n",
    "            batch_y = torch.autograd.Variable(batch_y)\n",
    "            #batch_y = batch_y.reshape(-1,1)\n",
    "            #print(batch_y)\n",
    "            #print(output)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            #print(loss.item())\n",
    "            #break\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # model.zero_grad()\n",
    "            #optimizer.zero_grad()\n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            output = model.forward(batch_X)\n",
    "            \n",
    "            #_, batch_y = batch_y.max(dim=1)            \n",
    "            batch_y = torch.autograd.Variable(batch_y)\n",
    "            #batch_y = batch_y.reshape(-1,1)\n",
    "            #print(output)\n",
    "            \n",
    "            loss = loss_fn(output, batch_y)            \n",
    "            val_loss += loss.item()\n",
    "            #break\n",
    "        \n",
    "        total_train_loss = train_loss / len(train_loader)\n",
    "        total_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(\"Epoch: {}, BCE Train Loss: {} Valid Loss {}\".format(epoch, total_train_loss, total_val_loss))\n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        #break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCE Train Loss: 0.6932063950432672 Valid Loss 0.6931546926498413\n",
      "Epoch: 2, BCE Train Loss: 0.6931547727849748 Valid Loss 0.6931546926498413\n",
      "Epoch: 3, BCE Train Loss: 0.6931547727849748 Valid Loss 0.6931546926498413\n",
      "Epoch: 4, BCE Train Loss: 0.6931547727849748 Valid Loss 0.6931546926498413\n",
      "Epoch: 5, BCE Train Loss: 0.6931547727849748 Valid Loss 0.6931546926498413\n",
      "Epoch: 6, BCE Train Loss: 0.6931547727849748 Valid Loss 0.6931546926498413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7463831b718d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#loss_fn = nn.BCEWithLogitsLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_tn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sample_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_sample_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-686f64b86687>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep-learning/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep-learning/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model = DNNClassifier(hidden_dim, input_size, output_size).to(device)\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "#loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_tn = train(model, train_sample_dl, var_sample_dl, num_epochs, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
