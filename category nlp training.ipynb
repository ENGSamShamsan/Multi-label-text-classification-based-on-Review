{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predict The News Category Hackathon</h2>\n",
    "\n",
    "From the beginning, since the first printed newspaper, every news that makes into a page has had a specific section allotted to it. Although pretty much everything changed in newspapers from the ink to the type of paper used, this proper categorization of news was carried over by generations and even to the digital versions of the newspaper. Newspaper articles are not limited to a few topics or subjects, it covers a wide range of interests from politics to sports to movies and so on. For long, this process of sectioning was done manually by people but now technology can do it without much effort. In this hackathon, Data Science and Machine Learning enthusiasts like you will use Natural Language Processing to predict which genre or category a piece of news will fall in to from the story.<br/>\n",
    "\n",
    "Size of training set: 7,628 records<br/>\n",
    "Size of test set: 2,748 records<br/>\n",
    "\n",
    "<b>FEATURES</b>:<br/>\n",
    "\n",
    "<b>STORY</b>:  A part of the main content of the article to be published as a piece of news.<br/>\n",
    "<b>SECTION</b>: The genre/category the STORY falls in.<br/>\n",
    "\n",
    "There are four distinct sections where each story may fall in to. The Sections are labelled as follows :<br/>\n",
    "\n",
    "Politics: 0 <br/>\n",
    "Technology: 1 <br/>\n",
    "Entertainment: 2 <br/>\n",
    "Business: 3 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/koushik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import operator\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "import re, string\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, scale, PolynomialFeatures\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, r2_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from torch.autograd import Variable\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "#import cufflinks as cf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#init_notebook_mode(connected=True)\n",
    "#cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"Data_Train.csv\", keep_default_na=False)\n",
    "test_csv = pd.read_csv(\"Data_Test.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 1)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 2)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def beautifyAllText(x):\n",
    "    if x != \"\":\n",
    "        text = BeautifulSoup(x, \"html.parser\").get_text() # Remove HTML tags\n",
    "        #text = text.lower().translate(str.maketrans('\\n',' ',string.punctuation))\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "        words = text.split() # Split string into words\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "        words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "        return \" \".join(words)\n",
    "        return text\n",
    "    return x\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    #dataset.sort_values(\"Review Text\", inplace = True) \n",
    "    #dataset.drop_duplicates(subset =\"Review Text\", keep=False,inplace=False)\n",
    "    #dataset = dataset.reset_index(drop=True)\n",
    "    #dataset['Review Text'] = dataset[\"Review Title\"].map(str) + \" \" + dataset['Review Text']\n",
    "    dataset['STORY'] = dataset[\"STORY\"].apply(beautifyAllText)\n",
    "    #dataset = dataset.drop(['Review Title'],axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 7628, test = 2748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the most painful was the huge reversal in ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How formidable is the opposition alliance amon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most Asian currencies were trading lower today...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to answer any question, click on ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In global markets, gold prices edged up today ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STORY  SECTION\n",
       "0  But the most painful was the huge reversal in ...        3\n",
       "1  How formidable is the opposition alliance amon...        0\n",
       "2  Most Asian currencies were trading lower today...        3\n",
       "3  If you want to answer any question, click on ‘...        1\n",
       "4  In global markets, gold prices edged up today ...        3"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = preprocess_data(train_csv.copy())\n",
    "test = preprocess_data(test_csv.copy())\n",
    "\n",
    "train_val = train[0:1000]\n",
    "\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train[\"Review Text\"], train[\"topic\"], random_state=33)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train), len(test)))\n",
    "#s = \" \".join(train[\"Review Text\"][0:10])\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But the most painful was the huge reversal in fee income, unheard of among private sector lenders. Essentially, it means that Yes Bank took it for granted that fees on structured loan deals will be paid and accounted for upfront on its books. As borrowers turned defaulters, the fees tied to these loan deals fell off the cracks. Gill has now vowed to shift to a safer accounting practice of amortizing fee income rather than booking these upfront.\\n\\n\\nGill’s move to mend past ways means that there will be no nasty surprises in the future. This is good news considering that investors love a clean image and loathe uncertainties.\\n\\n\\nBut there is no gain without pain and the promise of a strong and stable balance sheet comes with some sacrifices as well. Investors will have to give up the hopes of phenomenal growth, a promise made by Kapoor.'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv[\"STORY\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pain huge revers fee incom unheard among privat sector lender essenti mean ye bank took grant fee structur loan deal paid account upfront book borrow turn default fee tie loan deal fell crack gill vow shift safer account practic amort fee incom rather book upfront gill move mend past way mean nasti surpris futur good news consid investor love clean imag loath uncertainti gain without pain promis strong stabl balanc sheet come sacrific well investor give hope phenomen growth promis made kapoor'"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"STORY\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408.6368641845831, 318.79337461128694, 3771)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = train[\"STORY\"].str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECTION</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         STORY\n",
       "SECTION       \n",
       "0         1686\n",
       "1         2772\n",
       "2         1924\n",
       "3         1246"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('SECTION').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.groupby('topic').count()['Review Title'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8, title='Bar chart of Department Name', xTitle='Department Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X.fillna(\"unknown\", inplace=True)\n",
    "#test_X.fillna(\"unknown\", inplace=True)\n",
    "#train_val[\"Review Text\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "#def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048061\n"
     ]
    }
   ],
   "source": [
    "n = train.shape[0]\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,7), tokenizer=tokenize, stop_words='english')    \n",
    "\"\"\"\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\"\"\"\n",
    "\n",
    "vec = vec.fit(pd.concat([train[\"STORY\"], test[\"STORY\"]], axis=0))\n",
    "\n",
    "trn_term_doc = vec.transform(train[\"STORY\"])\n",
    "val_term_doc = vec.transform(train_val[\"STORY\"])\n",
    "test_term_doc = vec.transform(test[\"STORY\"])\n",
    "\n",
    "print(len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7628, 3048061)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(trn_term_doc.shape)\n",
    "print(type(trn_term_doc))\n",
    "#print(trn_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_preds = np.zeros((len(test), 4))\n",
    "#print(preds.shape)\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "#model = SVC(class_weight=None)\n",
    "#model = MultinomialNB()\n",
    "#model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "train_y_label = list(train[\"SECTION\"].values)\n",
    "val_y_label = list(train_val[\"SECTION\"].values)\n",
    "\n",
    "model = model.fit(trn_term_doc, train_y_label)\n",
    "train_score = model.score(trn_term_doc, train_y_label)\n",
    "print(train_score)\n",
    "\n",
    "test_score = model.score(val_term_doc, val_y_label)\n",
    "print(test_score)\n",
    "\n",
    "\n",
    "final_preds = model.predict(test_term_doc)\n",
    "\"\"\"\n",
    "for i, col in enumerate(label_cols):\n",
    "    print('fit', col)\n",
    "    m,r = get_mdl(binary_label[col])\n",
    "    final_preds[:,i] = m.predict_proba(test_term.multiply(r))[:,1]\n",
    "\n",
    "\"\"\"\n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "final_preds = np.zeros((len(test_X), len(label_cols)))\n",
    "print(final_preds.shape)\n",
    "\n",
    "for i, col in enumerate(label_cols):\n",
    "    print('fit', col)\n",
    "    m,r = get_mdl(binary_label[col])\n",
    "    final_preds[:,i] = m.predict_proba(test_term.multiply(r))[:,1]\n",
    "\n",
    "#print(preds)\n",
    "\n",
    "output = []\n",
    "for row in final_preds:\n",
    "    output.append(np.argmax(row))\n",
    "print(output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(pd.Series(final_preds), columns = [\"SECTION\"])\n",
    "display(predictions.head())\n",
    "predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
